{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T15:21:57.944805100Z",
     "start_time": "2024-04-22T15:21:57.490829100Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\__init__.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodulefinder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Module\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01malexnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdensenet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstochastic_depth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_presets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgiou_loss\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generalized_box_iou_loss\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpoolers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiScaleRoIAlign\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mps_roi_align\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ps_roi_align, PSRoIAlign\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mps_roi_pool\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mboxes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m box_area\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _log_api_usage_once\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mroi_align\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m roi_align\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# is not supported by ONNX tracing yet.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# that merges the levels to the right indices\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39munused\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_onnx_merge_levels\u001B[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torchvision\\ops\\roi_align.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Union\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcode_context\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m code_context\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:28\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecated\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdeprecated_func\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfx\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_symbolic_trace\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_fx_tracing\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexternal_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_compiling\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hashable, is_safe_constant, NP_SUPPORTED_MODULES\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torch\\_dynamo\\config.py:288\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m DEBUG_DIR_VAR_NAME \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n\u001B[0;32m    285\u001B[0m     debug_dir_root \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(  \u001B[38;5;66;03m# [@compile_ignored: debug]\u001B[39;00m\n\u001B[0;32m    286\u001B[0m         os\u001B[38;5;241m.\u001B[39menviron[DEBUG_DIR_VAR_NAME], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_compile_debug\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m     )\n\u001B[1;32m--> 288\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mis_fbcode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    289\u001B[0m     debug_dir_root \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(  \u001B[38;5;66;03m# [@compile_ignored: debug]\u001B[39;00m\n\u001B[0;32m    290\u001B[0m         tempfile\u001B[38;5;241m.\u001B[39mgettempdir(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_compile_debug\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    291\u001B[0m     )\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torch\\_dynamo\\config.py:279\u001B[0m, in \u001B[0;36mis_fbcode\u001B[1;34m()\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_fbcode\u001B[39m():\n\u001B[1;32m--> 279\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit_version\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Studia\\Sem 8\\SNSE\\Projekt3\\ssne-projekt-3\\venv\\Lib\\site-packages\\torch\\__init__.py:1938\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m   1935\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m   1936\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m-> 1938\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755548858ec011f6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-22T15:21:36.872364300Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc137f58256fd7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-22T15:21:36.876364400Z"
    }
   },
   "outputs": [],
   "source": [
    "class FastDataset(Dataset):\n",
    "    def __init__(self, data, labels, num_classes):\n",
    "        self.dataset = data\n",
    "        self.labels = labels\n",
    "        self.number_classes = num_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f2eac7a8df8a3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-22T15:21:16.462793100Z"
    }
   },
   "outputs": [],
   "source": [
    "# wersja bez augmentacji\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# wersja z augmentacją\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.2),\n",
    "    transforms.RandomVerticalFlip(0.2),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# wersja z augmentacją\n",
    "transform3 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "transform = transform1\n",
    "\n",
    "dataset = datasets.ImageFolder(\"train/\", transform=transform)\n",
    "classes = dataset.classes\n",
    "train_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "data_train = next(iter(train_loader))\n",
    "fast_dataset_train = FastDataset(data_train[0], data_train[1], num_classes=50)\n",
    "fast_train_loader = DataLoader(fast_dataset_train, batch_size = 128)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T06:58:34.272433400Z",
     "start_time": "2024-04-15T06:52:10.993386200Z"
    }
   },
   "id": "43e7860f3f9b00b9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1f4c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T05:22:32.082815400Z",
     "start_time": "2024-04-15T05:16:16.578640200Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transform1\n",
    "\n",
    "dataset = datasets.ImageFolder(\"train/\", transform=transform)\n",
    "train_size = int(0.8 * len(dataset)) \n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "classes = dataset.classes\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=len(train_set))\n",
    "data_train = next(iter(train_loader))\n",
    "fast_dataset_train = FastDataset(data_train[0], data_train[1], num_classes=50)\n",
    "fast_train_loader = DataLoader(fast_dataset_train, batch_size = 128)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True, num_workers=12)\n",
    "data_test = next(iter(test_loader))\n",
    "fast_dataset_test = FastDataset(data_test[0], data_test[1], num_classes=50)\n",
    "fast_test_loader = DataLoader(fast_dataset_test, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1566dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T06:58:34.912826900Z",
     "start_time": "2024-04-15T06:58:34.278426300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Net(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  (fc1): Linear(in_features=16384, out_features=2048, bias=True)\n  (dropout1): Dropout(p=0.4, inplace=False)\n  (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n  (dropout2): Dropout(p=0.4, inplace=False)\n  (fc3): Linear(in_features=2048, out_features=50, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy 50%\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(192)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(384)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.AvgPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 2048)\n",
    "        self.dropout1 = nn.Dropout(0.4)  \n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.dropout2 = nn.Dropout(0.4)  \n",
    "        self.fc3 = nn.Linear(2048, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn5(self.conv5(F.relu(self.bn4(self.conv4(F.relu(self.bn3(self.conv3(x))))))))))\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Apply dropout after the first fully connected layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Apply dropout after the second fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e313f208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T06:58:34.926770Z",
     "start_time": "2024-04-15T06:58:34.919828100Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956f586d3bbb8543",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T05:22:32.955932400Z",
     "start_time": "2024-04-15T05:22:32.846465500Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_accuracy(net, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = net(images).cpu()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trening z monitorowaniem accuracy\n",
    "test_accuracy_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for (inputs, labels) in fast_train_loader:\n",
    "        net.train()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    # net.eval()\n",
    "    # test_acc = test_accuracy(net, fast_test_loader)\n",
    "    # test_accuracy_history.append(test_acc)\n",
    "    net.train()\n",
    "    train_loss = running_loss / len(fast_train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.7f}, Test Accuracy: {1:.3f}%\")\n",
    "print('Finished Training')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.plot(test_accuracy_history)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Accuracy over Epochs')\n",
    "\n",
    "ax2.plot(train_loss_history)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Train Loss')\n",
    "ax2.set_title('Train Loss over Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "950c7c3c4296d9db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Accuracy of the network on the test images: %.3f %%' % (test_accuracy(net, fast_test_loader)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aed00ad852f4128"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data    \n",
    "        images = images.to(device)\n",
    "        outputs = net(images).cpu()   \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,accuracy))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60c7cc1246ba4920"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trenowanie modelu finalnego od nowa na wszystkim\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9d5e0317e65bfc8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\"train/\", transform=transform)\n",
    "train_size = int(len(dataset)-1) \n",
    "test_size = 1\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "classes = dataset.classes\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=len(train_set))\n",
    "data_train = next(iter(train_loader))\n",
    "fast_dataset_train = FastDataset(data_train[0], data_train[1], num_classes=50)\n",
    "fast_train_loader = DataLoader(fast_dataset_train, batch_size = 128)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T12:55:40.034133700Z",
     "start_time": "2024-04-15T12:48:10.475009Z"
    }
   },
   "id": "1b9551489250356"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0262797\n",
      "Epoch 2, Loss: 0.0225023\n",
      "Epoch 3, Loss: 0.0204499\n",
      "Epoch 4, Loss: 0.0189021\n",
      "Epoch 5, Loss: 0.0175066\n",
      "Epoch 6, Loss: 0.0161496\n",
      "Epoch 7, Loss: 0.0149356\n",
      "Epoch 8, Loss: 0.0137359\n",
      "Epoch 9, Loss: 0.0126461\n",
      "Epoch 10, Loss: 0.0115783\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for (inputs, labels) in fast_train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(fast_dataset_train):.7f}\")\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T13:16:21.507745800Z",
     "start_time": "2024-04-15T12:55:40.022036600Z"
    }
   },
   "id": "1ba7848a6bd74b1b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_all_data = datasets.ImageFolder(\"test_all/\", transform=test_transform)\n",
    "test_all_loader = torch.utils.data.DataLoader(test_all_data, batch_size=128, shuffle=False, num_workers=12)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_all_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = net(images).cpu()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "file_names = [os.path.basename(file_path) for file_path, _ in test_all_data.imgs]\n",
    "\n",
    "with open(\"grądziel_misztal.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for file_name, prediction in zip(file_names, predictions):\n",
    "        writer.writerow([file_name, prediction])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T13:16:59.307503600Z",
     "start_time": "2024-04-15T13:16:21.522710600Z"
    }
   },
   "id": "6cb2722f7c621de3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2011c4e60935dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T12:54:01.344552300Z",
     "start_time": "2024-04-14T12:54:01.336115600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
